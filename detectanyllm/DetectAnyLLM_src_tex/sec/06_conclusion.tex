\section{Conclusion}
In this study, we have introduced a novel optimization strategy, \textit{Direct Discrepancy Learning (DDL)}, and developed a unified detection framework named \textit{DetectAnyLLM}.
%
Our approach enables the scoring model to acquire task-oriented knowledge by directly leveraging discrepancy signals and achieves high-precision detection through a technique we call \textit{reference clustering}.
%
We also proposed \textit{MIRAGE}, a comprehensive benchmark dataset that spans a wide range of text domains, the most advanced LLMs, and generation tasks.
%
To thoroughly evaluate detector performance, we assessed DetectAnyLLM and existing MGTD methods under two settings: \textit{Disjoint-Input Generation} and \textit{Shared-Input Generation}.
%
Experimental results on both MIRAGE and previously established test sets demonstrate that DetectAnyLLM significantly outperforms existing MGTD methods, establishing a new state-of-the-art in this domain.