\section{Related Work}
\subsection{Zero-shot Detector}
Previous MGTD research emphasized zero-shot detection due to concerns about overfitting during training~\cite{outdomain, overfit}. 
%
Early methods like GLTR~\cite{entropy} leveraged text entropy to detect machine content, while others used likelihood- or ranking-based approaches~\cite{likelihood, logrank}.
%
Recently, Detect-GPT~\cite{detectgpt} has been proposed, which distinguishes MGT from HWT using \textit{perturbation}, \textit{scoring}, and \textit{probability curvature estimation}, providing a novel view for MGTD.
%
%
DetectLLM~\cite{lrrandnpr} further improves the performance by adding Log-Rank Score.
%
Fast-DetectGPT~\cite{fastdetectgpt} improves the perturbation step, significantly accelerating the detection process without sacrificing performance.
%
Despite progress, zero-shot methods remain constrained by their dependence on the scoring model’s output distribution, as shown by Glimpse~\cite{glimpse}, which demonstrated performance improvement through stronger scoring models.


\subsection{Training-based Detector}
The training-based detector fine-tunes the scoring model on specific training data.
%
An early representative work is RoBERTa-OpenAI-Detector~\cite{likelihood}, the researchers fine-tune RoBERTa~\cite{roberta} models using GPT-2~\cite{gpt2}-generated data, performing well on detecting GPT-generated text.
%
RADAR~\cite{radar} incorporates adversarial training~\cite{gan} to enhance MGTD robustness and uses PPO~\cite{ppo} to optimize the generator, improving the generator-generated-text's similarity to HWT. 
%
More recently, ImBD~\cite{imbd} utilizes DPO~\cite{dpo} to optimize the scoring model in Fast-DetectGPT~\cite{fastdetectgpt}, aiming to help the scoring model better capture the style features of the training data.

Despite these advancements, most methods simply focus on training the scoring model to approximate the source model’s distribution rather than developing a dedicated MGTD detector. 
%
This introduces constraints to the scoring model during training, which are detrimental to the MGTD task.
%
By freeing the scoring model from its language model's identity constraint, we enable it to \textbf{learn to be a detector rather than another language model}, thereby enhancing its generalization ability.


\subsection{MGTD Benchmark}
Early benchmarks like Turingbench~\cite{turingbench} focused on news articles generated by neural models, while the emergence of ChatGPT~\cite{instructGPT} shifted attention to LLM-generated text, exemplified by MGTBench\cite{mgtbench} and HC3~\cite{hc3}. 
%
Later efforts such as MAGE~\cite{mage}, MULTITuDE~\cite{multitude}, and M4~\cite{m4bench} explored open-domain and multilingual detection.
%
RAID~\cite{raid} novelly introduced decoding strategy considerations to strengthen evaluation robustness, while DetectRL~\cite{detectrl} examined vulnerabilities from a writing-attack perspective.
%
However, most of these benchmarks rely on open-source models (indicating limited variousity) and focus mainly on MGT, overlooking more common real-world applications involving MRT, thus limits their applicability to real-world contexts.

HART~\cite{hart} marked progress by incorporating both MGT and MRT using six advanced LLMs (only four commercial LLMs), but it remains limited in generator diversity and domain scope.
%
In this study, we scale up the number of generators to 17, where 13 are commercial LLMs and 4 are advanced open-source LLms, covering nearly all major LLMs used in real-world applications.
%
Moreover, we sample HWT from five distinct domains and generate both MGT and MRT, ensuring a more comprehensive and representative evaluation.
%
To advance MGTD research and enable fairer comparisons, we advocate for the adoption of a unified benchmark to ensure consistency in evaluation standards. We hope that our benchmark will serve as a valuable step toward achieving this goal.