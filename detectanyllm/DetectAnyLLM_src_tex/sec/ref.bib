% --------------------------------------------------
% llms
% --------------------------------------------------
@article{qwen2,
  title={Qwen2 Technical Report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={CoRR},
  year={2024}
}

@article{qwen2.5,
  title={Qwen2.5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@artic{llama3,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{gpt4o,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{gpto1,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}

@article{deepseekr1,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{deepseekv3,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@software{gpt-neo,
  author       = {Black, Sid and Gao, Leo and Wang, Phil and Leahy, Connor and Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow}},
  month        = {mar},
  year         = {2021},
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@software{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  year={2019}
}

@software{claude,
  title={Model Card Addendum: Claude 3.5 Haiku and Upgraded Claude 3.5 Sonnet},
  author={Anthropic},
  year={2024},
  url={https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf}
}

@article{gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

% --------------------------------------------------
% survey
% --------------------------------------------------

@inproceedings{survey_automatic,
  title={Automatic Detection of Machine Generated Text: A Critical Survey},
  author={Jawahar, Ganesh and Abdul-Mageed, Muhammad and Laks Lakshmanan, VS},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={2296--2309},
  year={2020}
}

@article{survey_necessity_methods_futuredirect,
  title={A survey on LLM-generated text detection: Necessity, methods, and future directions},
  author={Wu, Junchao and Yang, Shu and Zhan, Runzhe and Yuan, Yulin and Chao, Lidia Sam and Wong, Derek Fai},
  journal={Computational Linguistics},
  pages={1--66},
  year={2025},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{survey_science,
  title={The science of detecting LLM-generated text},
  author={Tang, Ruixiang and Chuang, Yu-Neng and Hu, Xia},
  journal={Communications of the ACM},
  volume={67},
  number={4},
  pages={50--59},
  year={2024},
  publisher={ACM New York, NY, USA}
}


% --------------------------------------------------
% optim methods
% --------------------------------------------------

@article{dpo,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53728--53741},
  year={2023}
}

@inproceedings{orpo,
  title={ORPO: Monolithic Preference Optimization without Reference Model},
  author={Hong, Jiwoo and Lee, Noah and Thorne, James},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={11170--11189},
  year={2024}
}

@article{sft,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{gan,
  title={Generative adversarial nets},
  author={Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

% --------------------------------------------------
% benchmarks
% --------------------------------------------------

@inproceedings{mgtbench,
  title={Mgtbench: Benchmarking machine-generated text detection},
  author={He, Xinlei and Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Zhang, Yang},
  booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
  pages={2251--2265},
  year={2024}
}

@inproceedings{m4bench,
  title={M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection},
  author={Wang, Yuxia and Mansurov, Jonibek and Ivanov, Petar and Su, Jinyan and Shelmanov, Artem and Tsvigun, Akim and Whitehouse, Chenxi and Afzal, Osama Mohammed and Mahmoud, Tarek and Sasaki, Toru and others},
  booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1369--1407},
  year={2024}
}

@article{detectrl,
  title={Detectrl: Benchmarking llm-generated text detection in real-world scenarios},
  author={Wu, Junchao and Zhan, Runzhe and Wong, Derek and Yang, Shu and Yang, Xinyi and Yuan, Yulin and Chao, Lidia},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={100369--100401},
  year={2024}
}

@inproceedings{fakenews,
  title={" Liar, Liar Pants on Fire": A New Benchmark Dataset for Fake News Detection},
  author={Wang, William Yang},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  year={2017},
  organization={Association for Computational Linguistics}
}

@inproceedings{raid,
  title={RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors},
  author={Dugan, Liam and Hwang, Alyssa and Trhl{\'\i}k, Filip and Zhu, Andrew and Ludan, Josh Magnus and Xu, Hainiu and Ippolito, Daphne and Callison-Burch, Chris},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={12463--12492},
  year={2024}
}

@article{hc3,
  title={How close is chatgpt to human experts? comparison corpus, evaluation, and detection},
  author={Guo, Biyang and Zhang, Xin and Wang, Ziyuan and Jiang, Minqi and Nie, Jinran and Ding, Yuxuan and Yue, Jianwei and Wu, Yupeng},
  journal={arXiv preprint arXiv:2301.07597},
  year={2023}
}

@misc{hart,
      title={Decoupling Content and Expression: Two-Dimensional Detection of AI-Generated Text}, 
      author={Guangsheng Bao and Lihua Rong and Yanbin Zhao and Qiji Zhou and Yue Zhang},
      year={2025},
      eprint={2503.00258},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.00258}, 
}

@inproceedings{turingbench,
  title={TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation},
  author={Uchendu, Adaku and Ma, Zeyu and Le, Thai and Zhang, Rui and Lee, Dongwon},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={2001--2016},
  year={2021}
}

@inproceedings{mage,
  title={MAGE: Machine-generated Text Detection in the Wild},
  author={Li, Yafu and Li, Qintong and Cui, Leyang and Bi, Wei and Wang, Zhilin and Wang, Longyue and Yang, Linyi and Shi, Shuming and Zhang, Yue},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={36--53},
  year={2024}
}

@inproceedings{multitude,
  title={MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark},
  author={Macko, Dominik and Moro, Robert and Uchendu, Adaku and Lucas, Jason and Yamashita, Michiharu and Pikuliak, Mat{\'u}{\v{s}} and Srba, Ivan and Le, Thai and Lee, Dongwon and Simko, Jakub and others},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9960--9987},
  year={2023}
}

% --------------------------------------------------
% datasets
% --------------------------------------------------

@inproceedings{bigpatent,
  title={BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization},
  author={Sharma, Eva and Li, Chen and Wang, Lu},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2204--2213},
  year={2019}
}

@inproceedings{xlsum,
  title={XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages},
  author={Hasan, Tahmid and Bhattacharjee, Abhik and Islam, Md Saiful and Mubasshir, Kazi and Li, Yuan-Fang and Kang, Yong-Bin and Rahman, M Sohel and Shahriyar, Rifat},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={4693--4703},
  year={2021}
}

@inproceedings{xsum,
    title = "Don`t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
    author = "Narayan, Shashi  and
      Cohen, Shay B.  and
      Lapata, Mirella",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1206/",
    doi = "10.18653/v1/D18-1206",
    pages = "1797--1807",
    abstract = "We introduce {\textquotedblleft}extreme summarization{\textquotedblright}, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question {\textquotedblleft}What is the article about?{\textquotedblright}. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article`s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans."
}

@inproceedings{pubmedqa,
    title = "{P}ub{M}ed{QA}: A Dataset for Biomedical Research Question Answering",
    author = "Jin, Qiao  and
      Dhingra, Bhuwan  and
      Liu, Zhengping  and
      Cohen, William  and
      Lu, Xinghua",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1259/",
    doi = "10.18653/v1/D19-1259",
    pages = "2567--2577",
    abstract = "We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1{\%} accuracy, compared to single human performance of 78.0{\%} accuracy and majority-baseline of 55.2{\%} accuracy, leaving much room for improvement. PubMedQA is publicly available at \url{https://pubmedqa.github.io}."
}

@inproceedings{writting_prompts,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1082/",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one."
}

% --------------------------------------------------
% methods
% --------------------------------------------------

@inproceedings{detectgpt,
  title={Detectgpt: Zero-shot machine-generated text detection using probability curvature},
  author={Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={24950--24962},
  year={2023},
  organization={PMLR}
}

@article{roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{likelihood,
  title={Release strategies and the social impacts of language models},
  author={Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and others},
  journal={arXiv preprint arXiv:1908.09203},
  year={2019}
}

@inproceedings{fastdetectgpt,
  title={Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature},
  author={Bao, Guangsheng and Zhao, Yanbin and Teng, Zhiyang and Yang, Linyi and Zhang, Yue},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{imbd,
  title={Imitate Before Detect: Aligning Machine Stylistic Preference for Machine-Revised Text Detection},
  author={Chen, Jiaqi and Zhu, Xiaoye and Liu, Tianyang and Chen, Ying and Xinhui, Chen and Yuan, Yiwen and Leong, Chak Tou and Li, Zuchao and Tang, Long and Zhang, Lei and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={22},
  pages={23559--23567},
  year={2025}
}

@article{logrank,
  title={Automatic detection of generated text is easiest when humans are fooled},
  author={Ippolito, Daphne and Duckworth, Daniel and Callison-Burch, Chris and Eck, Douglas},
  journal={arXiv preprint arXiv:1911.00650},
  year={2019}
}

@inproceedings{entropy,
  title={GLTR: Statistical Detection and Visualization of Generated Text},
  author={Gehrmann, Sebastian and Strobelt, Hendrik and Rush, Alexander M},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={111--116},
  year={2019}
}

@inproceedings{lrrandnpr,
  title={DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text},
  author={Su, Jinyan and Zhuo, Terry and Wang, Di and Nakov, Preslav},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={12395--12412},
  year={2023}
}

@inproceedings{dna-gpt,
  title={DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text},
  author={Yang, Xianjun and Cheng, Wei and Wu, Yue and Petzold, Linda Ruth and Wang, William Yang and Chen, Haifeng},
  booktitle={ICLR},
  year={2024}
}

@article{pawn,
  title={Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection},
  author={Miralles-Gonz{\'a}lez, Pablo and Huertas-Tato, Javier and Mart{\'\i}n, Alejandro and Camacho, David},
  journal={arXiv preprint arXiv:2501.03940},
  year={2025}
}

@article{indistinguish_1,
  title={Is GPT-3 text indistinguishable from human text? scarecrow: A framework for scrutinizing machine text},
  author={Dou, Yao and Forbes, Maxwell and Koncel-Kedziorski, Rik and Smith, Noah A and Choi, Yejin},
  journal={arXiv preprint arXiv:2107.01294},
  year={2021}
}

@article{indistinguish_2,
  title={Comparing abstractive summaries generated by ChatGPT to real summaries through blinded reviewers and text classification algorithms},
  author={Soni, Mayank and Wade, Vincent},
  journal={arXiv preprint arXiv:2303.17650},
  year={2023}
}

@article{overfit,
  title={Real or fake? learning to discriminate machine from human generated text},
  author={Bakhtin, Anton and Gross, Sam and Ott, Myle and Deng, Yuntian and Ranzato, Marc'Aurelio and Szlam, Arthur},
  journal={arXiv preprint arXiv:1906.03351},
  year={2019}
}

@inproceedings{outdomain,
  title={Deepfake text detection: Limitations and opportunities},
  author={Pu, Jiameng and Sarwar, Zain and Abdullah, Sifat Muhammad and Rehman, Abdullah and Kim, Yoonjin and Bhattacharya, Parantapa and Javed, Mobin and Viswanath, Bimal},
  booktitle={2023 IEEE symposium on security and privacy (SP)},
  pages={1613--1630},
  year={2023},
  organization={IEEE}
}

@article{radar,
  title={Radar: Robust ai-text detection via adversarial learning},
  author={Hu, Xiaomeng and Chen, Pin-Yu and Ho, Tsung-Yi},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={15077--15095},
  year={2023}
}

@article{instructGPT,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{glimpse,
  title={Glimpse: Enabling White-Box Methods to Use Proprietary Models for Zero-Shot LLM-Generated Text Detection},
  author={Bao, Guangsheng and Zhao, Yanbin and He, Juncai and Zhang, Yue},
  journal={arXiv preprint arXiv:2412.11506},
  year={2024}
}